{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SwitchableTokenizer Experiments (Training from Scratch)\n",
        "\n",
        "This notebook runs the SwitchableTokenizer experiments with models trained from scratch instead of fine-tuning.\n",
        "This allows evaluating how well the switchable tokenizer approach works with freshly initialized models.\n",
        "\n",
        "## Overview of Experiments\n",
        "\n",
        "1. **Experiment 1: Feasibility and Performance** - Compares the switchable tokenizer model with monolingual models\n",
        "2. **Experiment 2: Comparison vs. Concatenated Vocab** - Compares the switchable tokenizer with a concatenated vocabulary\n",
        "3. **Experiment 3: Multilingual Baseline** - Compares against a standard multilingual tokenizer\n",
        "4. **Experiment 4: Context Sensitivity** - Analyzes how token probabilities shift based on language context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "First, let's set up our environment by cloning the repository and installing dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/hardesttype/switch-tokenizer.git\n",
        "!cd switch-tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if running in Colab and install required packages\n",
        "IN_COLAB = 'google.colab' in str(get_ipython())\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Google Colab\")\n",
        "    !pip install -q datasets huggingface_hub dotenv\n",
        "else:\n",
        "    print(\"Not running in Google Colab\")\n",
        "    !pip install -q datasets huggingface_hub dotenv torch transformers datasets tokenizers matplotlib seaborn tqdm numpy pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up Hugging Face token from Colab secrets\n",
        "# Note: You need to add your HF token as a secret named 'hfToken' in Colab\n",
        "# Go to: Colab menu -> Secrets -> Add new secret\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        hf_token = userdata.get('hfToken')\n",
        "        if hf_token:\n",
        "            print(\"✅ Hugging Face token loaded from Colab secrets\")\n",
        "            # Set the token as an environment variable\n",
        "            import os\n",
        "            os.environ[\"HF_TOKEN\"] = hf_token\n",
        "        else:\n",
        "            print(\"❌ HF token not found in Colab secrets. Please add it via the Colab menu: Secrets -> Add new secret\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error accessing Colab secrets: {e}\")\n",
        "        print(\"Please add your Hugging Face token as a secret named 'hfToken' via the Colab menu: Secrets -> Add new secret\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add the repository to the Python path\n",
        "import sys\n",
        "sys.path.append('/content/switch-tokenizer')\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Import common libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import set_seed\n",
        "\n",
        "# Check for GPU availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 1: Feasibility and Performance\n",
        "\n",
        "This experiment compares the performance of a model using the switchable tokenizer against individual monolingual models of similar size. It evaluates perplexity on held-out test data for each language.\n",
        "\n",
        "We'll train the models from scratch instead of fine-tuning pre-trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary modules for Experiment 1\n",
        "from experiments.experiment1_feasibility import main as experiment1_main\n",
        "import argparse\n",
        "\n",
        "# Set up argument parser for Experiment 1\n",
        "def run_experiment1(data_limit=500, epochs=1, batch_size=4, seed=42,\n",
        "                   en_dataset=\"wikimedia/wikipedia\", en_subset=\"20231101.en\",\n",
        "                   ru_dataset=\"wikimedia/wikipedia\", ru_subset=\"20231101.ru\",\n",
        "                   learning_rate=5e-5, max_seq_length=128,\n",
        "                   base_model=\"gpt2-medium\", output_dir=\"./experiment1_from_scratch_output\",\n",
        "                   first_shard_only=False, upload_to_hub=False, hub_repo_id=None):\n",
        "    # Save original sys.argv\n",
        "    orig_argv = sys.argv.copy()\n",
        "    \n",
        "    # Set new sys.argv with from_scratch flag\n",
        "    sys.argv = ['experiment1_feasibility.py', \n",
        "                '--from_scratch',\n",
        "                f'--data_limit={data_limit}',\n",
        "                f'--epochs={epochs}',\n",
        "                f'--batch_size={batch_size}',\n",
        "                f'--seed={seed}',\n",
        "                f'--output_dir={output_dir}',\n",
        "                f'--en_dataset={en_dataset}',\n",
        "                f'--en_subset={en_subset}',\n",
        "                f'--ru_dataset={ru_dataset}',\n",
        "                f'--ru_subset={ru_subset}',\n",
        "                f'--learning_rate={learning_rate}',\n",
        "                f'--max_seq_length={max_seq_length}',\n",
        "                f'--base_model={base_model}']\n",
        "    \n",
        "    # Add first_shard_only flag if enabled\n",
        "    if first_shard_only:\n",
        "        sys.argv.append('--first_shard_only')\n",
        "        \n",
        "    # Add Hugging Face upload options if enabled\n",
        "    if upload_to_hub and hub_repo_id:\n",
        "        sys.argv.append('--upload_to_hub')\n",
        "        sys.argv.append(f'--hub_repo_id={hub_repo_id}')\n",
        "    \n",
        "    print(f\"Running with command: {' '.join(sys.argv)}\\n\")\n",
        "    \n",
        "    # Run the experiment\n",
        "    try:\n",
        "        experiment1_main()\n",
        "    finally:\n",
        "        # Restore original sys.argv\n",
        "        sys.argv = orig_argv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Experiment 1 with small data for quicker execution in Colab\n",
        "# Adjust parameters as needed based on your computational resources\n",
        "run_experiment1(\n",
        "    data_limit=20_000,           # Use smaller dataset for faster execution\n",
        "    epochs=1,                 # Just one epoch for demonstration\n",
        "    batch_size=4,             # Small batch size for memory efficiency\n",
        "    en_dataset=\"wikimedia/wikipedia\",  # Source dataset for English\n",
        "    en_subset=\"20231101.en\",   # English dataset subset\n",
        "    ru_dataset=\"wikimedia/wikipedia\",  # Source dataset for Russian\n",
        "    ru_subset=\"20231101.ru\",   # Russian dataset subset\n",
        "    base_model=\"gpt2-medium\",  # Use smaller model for faster training\n",
        "    learning_rate=1e-4,       # Slightly higher learning rate for from-scratch training\n",
        "    max_seq_length=128,        # Shorter sequences for faster training\n",
        "    first_shard_only=True,    # Use only the first shard (train-00000-of-*) instead of counting examples\n",
        "    # Uncomment and update these lines to enable HF Hub upload\n",
        "    upload_to_hub=True,       # Enable uploading to Hugging Face Hub\n",
        "    hub_repo_id=\"hardesttype/switch-tokenizer-exp-1\"  # Replace with your repo ID\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 2: Comparison vs. Concatenated Vocab\n",
        "\n",
        "This experiment compares the performance of the switchable tokenizer model against a model using a concatenated vocabulary. It evaluates both perplexity and parameter efficiency.\n",
        "\n",
        "We'll train both models from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up function to run Experiment 2\n",
        "from experiments.experiment2_concatenated_vocab import main as experiment2_main\n",
        "\n",
        "def run_experiment2(data_limit=500, epochs=1, batch_size=4, seed=42,\n",
        "                   en_dataset=\"wikimedia/wikipedia\", en_subset=\"20231101.en\",\n",
        "                   ru_dataset=\"wikimedia/wikipedia\", ru_subset=\"20231101.ru\",\n",
        "                   learning_rate=5e-5, max_seq_length=128,\n",
        "                   base_model=\"gpt2-medium\", output_dir=\"./experiment2_from_scratch_output\",\n",
        "                   first_shard_only=False):\n",
        "    # Save original sys.argv\n",
        "    orig_argv = sys.argv.copy()\n",
        "    \n",
        "    # Set new sys.argv with from_scratch flag\n",
        "    sys.argv = ['experiment2_concatenated_vocab.py', \n",
        "                '--from_scratch',\n",
        "                f'--data_limit={data_limit}',\n",
        "                f'--epochs={epochs}',\n",
        "                f'--batch_size={batch_size}',\n",
        "                f'--seed={seed}',\n",
        "                f'--output_dir={output_dir}',\n",
        "                f'--en_dataset={en_dataset}',\n",
        "                f'--en_subset={en_subset}',\n",
        "                f'--ru_dataset={ru_dataset}',\n",
        "                f'--ru_subset={ru_subset}',\n",
        "                f'--learning_rate={learning_rate}',\n",
        "                f'--max_seq_length={max_seq_length}',\n",
        "                f'--base_model={base_model}']\n",
        "    \n",
        "    # Add first_shard_only flag if enabled\n",
        "    if first_shard_only:\n",
        "        sys.argv.append('--first_shard_only')\n",
        "    \n",
        "    print(f\"Running with command: {' '.join(sys.argv)}\\n\")\n",
        "    \n",
        "    # Run the experiment\n",
        "    try:\n",
        "        experiment2_main()\n",
        "    finally:\n",
        "        # Restore original sys.argv\n",
        "        sys.argv = orig_argv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Experiment 2\n",
        "run_experiment2(\n",
        "    data_limit=100,           # Use smaller dataset for faster execution\n",
        "    epochs=1,                 # Just one epoch for demonstration\n",
        "    batch_size=4,             # Small batch size for memory efficiency\n",
        "    en_dataset=\"wikimedia/wikipedia\",  # Source dataset for English\n",
        "    ru_dataset=\"wikimedia/wikipedia\",  # Source dataset for Russian\n",
        "    base_model=\"gpt2-medium\",  # Use smaller model for faster training\n",
        "    learning_rate=1e-4,       # Slightly higher learning rate for from-scratch training\n",
        "    max_seq_length=64,        # Shorter sequences for faster training\n",
        "    first_shard_only=True     # Use only the first shard (train-00000-of-*) instead of counting examples\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 3: Multilingual Baseline\n",
        "\n",
        "This experiment compares the switchable tokenizer against a standard multilingual tokenizer baseline. It evaluates tokenization efficiency and model perplexity.\n",
        "\n",
        "We'll train both models from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up function to run Experiment 3\n",
        "from experiments.experiment3_multilingual_baseline import main as experiment3_main\n",
        "\n",
        "def run_experiment3(data_limit=500, epochs=1, batch_size=4, seed=42,\n",
        "                   en_dataset=\"wikimedia/wikipedia\", en_subset=\"20231101.en\",\n",
        "                   ru_dataset=\"wikimedia/wikipedia\", ru_subset=\"20231101.ru\",\n",
        "                   en_tokenizer=\"gpt2\", ru_tokenizer=\"ai-forever/ruGPT-3.5-13B\",\n",
        "                   learning_rate=5e-5, max_seq_length=128,\n",
        "                   base_model=\"gpt2-medium\", output_dir=\"./experiment3_from_scratch_output\",\n",
        "                   first_shard_only=False):\n",
        "    # Save original sys.argv\n",
        "    orig_argv = sys.argv.copy()\n",
        "    \n",
        "    # Set new sys.argv with from_scratch flag\n",
        "    sys.argv = ['experiment3_multilingual_baseline.py', \n",
        "                '--from_scratch',\n",
        "                f'--data_limit={data_limit}',\n",
        "                f'--epochs={epochs}',\n",
        "                f'--batch_size={batch_size}',\n",
        "                f'--seed={seed}',\n",
        "                f'--output_dir={output_dir}',\n",
        "                f'--en_dataset={en_dataset}',\n",
        "                f'--en_subset={en_subset}',\n",
        "                f'--ru_dataset={ru_dataset}',\n",
        "                f'--ru_subset={ru_subset}',\n",
        "                f'--en_tokenizer={en_tokenizer}',\n",
        "                f'--ru_tokenizer={ru_tokenizer}',\n",
        "                f'--learning_rate={learning_rate}',\n",
        "                f'--max_seq_length={max_seq_length}',\n",
        "                f'--base_model={base_model}']\n",
        "    \n",
        "    # Add first_shard_only flag if enabled\n",
        "    if first_shard_only:\n",
        "        sys.argv.append('--first_shard_only')\n",
        "    \n",
        "    print(f\"Running with command: {' '.join(sys.argv)}\\n\")\n",
        "    \n",
        "    # Run the experiment\n",
        "    try:\n",
        "        experiment3_main()\n",
        "    finally:\n",
        "        # Restore original sys.argv\n",
        "        sys.argv = orig_argv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Experiment 3\n",
        "run_experiment3(\n",
        "    data_limit=100,           # Use smaller dataset for faster execution\n",
        "    epochs=1,                 # Just one epoch for demonstration\n",
        "    batch_size=4,             # Small batch size for memory efficiency\n",
        "    en_dataset=\"wikimedia/wikipedia\",  # Source dataset for English\n",
        "    ru_dataset=\"wikimedia/wikipedia\",  # Source dataset for Russian\n",
        "    en_tokenizer=\"gpt2\",      # English tokenizer\n",
        "    ru_tokenizer=\"ai-forever/ruGPT-3.5-13B\",  # Russian tokenizer\n",
        "    base_model=\"gpt2-medium\",  # Use smaller model for faster training\n",
        "    learning_rate=1e-4,       # Slightly higher learning rate for from-scratch training\n",
        "    max_seq_length=64,        # Shorter sequences for faster training\n",
        "    first_shard_only=True     # Use only the first shard (train-00000-of-*) instead of counting examples\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 4: Context Sensitivity Analysis\n",
        "\n",
        "This experiment analyzes how token probabilities shift based on language context. It specifically examines how the model learns to interpret token IDs differently depending on the language context.\n",
        "\n",
        "We'll use models trained from scratch to evaluate this language-specific behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up function to run Experiment 4\n",
        "from experiments.experiment4_context_sensitivity import main as experiment4_main\n",
        "\n",
        "def run_experiment4(model_dir, tokenizer_dir, num_test_tokens=50, num_prompts=3, seed=42, output_dir=\"./experiment4_from_scratch_output\"):\n",
        "    # Save original sys.argv\n",
        "    orig_argv = sys.argv.copy()\n",
        "    \n",
        "    # Set new sys.argv with from_scratch flag\n",
        "    sys.argv = ['experiment4_context_sensitivity.py',\n",
        "                '--from_scratch',\n",
        "                f'--model_dir={model_dir}',\n",
        "                f'--tokenizer_dir={tokenizer_dir}',\n",
        "                f'--output_dir={output_dir}',\n",
        "                f'--num_test_tokens={num_test_tokens}',\n",
        "                f'--num_prompts={num_prompts}',\n",
        "                f'--seed={seed}',\n",
        "                f'--device={device}']\n",
        "    \n",
        "    print(f\"Running with command: {' '.join(sys.argv)}\\n\")\n",
        "    \n",
        "    # Run the experiment\n",
        "    try:\n",
        "        experiment4_main()\n",
        "    finally:\n",
        "        # Restore original sys.argv\n",
        "        sys.argv = orig_argv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Experiment 4 (requires a trained model from previous experiments)\n",
        "# For example, use the model trained in Experiment 1\n",
        "model_dir = \"./experiment1_from_scratch_output/switchable_model/final_model\"\n",
        "tokenizer_dir = \"./experiment1_from_scratch_output/switchable_model/final_tokenizer\"\n",
        "\n",
        "run_experiment4(\n",
        "    model_dir=model_dir,\n",
        "    tokenizer_dir=tokenizer_dir,\n",
        "    num_test_tokens=20,       # Use fewer tokens for faster analysis\n",
        "    num_prompts=2,            # Test with fewer prompts per token\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrates how to run the SwitchableTokenizer experiments with models trained from scratch instead of fine-tuning pre-trained models.\n",
        "\n",
        "Training from scratch allows us to evaluate how well the switchable tokenizer approach works without relying on knowledge already embedded in pre-trained models. This is particularly important for:\n",
        "\n",
        "1. Understanding the inherent capabilities of the switchable tokenizer architecture\n",
        "2. Evaluating tokenization efficiency with a clean model\n",
        "3. Comparing parameter efficiency without pre-training bias\n",
        "4. Analyzing how models learn context-sensitive token interpretations from scratch\n",
        "\n",
        "Note that these experiments can be computationally intensive. In Google Colab, we use reduced dataset sizes and epochs to complete the experiments within the available GPU time limits.\n",
        "\n",
        "For even faster experimentation, we're using the `first_shard_only` option which loads only the first shard (train-00000) of each dataset rather than counting examples. This significantly reduces data loading time while still providing consistent samples across experiments."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
